{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import re\n",
    "import nltk \n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinde 50000 adet cümle mevcut.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  One of the other reviewers has mentioned that ...       1\n",
       "1  A wonderful little production. <br /><br />The...       1\n",
       "2  I thought this was a wonderful way to spend ti...       1\n",
       "3  Basically there's a family where a little boy ...       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veri setinin yüklenmesi ve örnek veri\n",
    "dataset = pd.read_csv(\"data.csv\",delimiter=\";\",header=None,names=[\"Review\",\"Rating\"])\n",
    "print(\"Verisetinde {} adet cümle mevcut.\".format(len(dataset)))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. \n",
      "\n",
      "\n",
      "one of the other review ha mention that after watch just 1 oz episod you ll be hook they are right as thi is exactli what happen with me br br the first thing that struck me about oz wa it brutal and unflinch scene of violenc which set in right from the word go trust me thi is not show for the faint heart or timid thi show pull no punch with regard to drug sex or violenc it is hardcor in the classic use of the word br br it is call oz as that is the nicknam given to the oswald maximum secur state penitentari it focus mainli on emerald citi an experiment section of the prison where all the cell have glass front and face inward so privaci is not high on the agenda em citi is home to mani aryan muslim gangsta latino christian italian irish and more so scuffl death stare dodgi deal and shadi agreement are never far away br br would say the main appeal of the show is due to the fact that it goe where other show wouldn dare forget pretti pictur paint for mainstream audienc forget charm forget romanc oz doesn mess around the first episod ever saw struck me as so nasti it wa surreal couldn say wa readi for it but as watch more develop tast for oz and got accustom to the high level of graphic violenc not just violenc but injustic crook guard who ll be sold out for nickel inmat who ll kill on order and get away with it well manner middl class inmat be turn into prison bitch due to their lack of street skill or prison experi watch oz you may becom comfort with what is uncomfort view that if you can get in touch with your darker side \n"
     ]
    }
   ],
   "source": [
    "#Veri ön işleme\n",
    "#Ön işleme öncesi örnek cümle\n",
    "print(dataset['Review'].values[0],\"\\n\\n\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Stemmer nesnesi oluşturulması\n",
    "porter = PorterStemmer() \n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#büyük harflerin küçük harfe çevrilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "\n",
    "# Özel karakterlerin(noktalama işareti vs) çıkartılması\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\W\", \" \", x))\n",
    "\n",
    "# tek karakterlerin boşluk ile değiştirilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", x))\n",
    "\n",
    "# en baştan tek kalan karakterlerin çıkartılması\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", x))\n",
    "\n",
    "# Birden fazla boşluğun tek boşlukla değiştirilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "\n",
    "# b öneklerinin silinmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"^b\\s+\", \" \", x))\n",
    "\n",
    "#fazladan boşlukların temizlenmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.strip())\n",
    "\n",
    "#stemming işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: stemSentence(x))\n",
    "\n",
    "data = dataset['Review'].values.tolist()\n",
    "target = dataset['Rating'].values.tolist()\n",
    "\n",
    "#Ön işleme sonrası aynı cümle\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 adet cümle eğitim için kullanılacak.\n",
      "5000 adet cümle test için kullanılacak.\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin eğitim ve test olarak ayrılması %90 Eğitim %10 test\n",
    "ratio = int(len(data) * .90)\n",
    "x_train, y_train = data[:ratio], target[:ratio]\n",
    "y_train = np.array(y_train)\n",
    "x_test, y_test   = data[ratio:], target[ratio:]\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"{} adet cümle eğitim için kullanılacak.\".format(len(x_train)))\n",
    "print(\"{} adet cümle test için kullanılacak.\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öncesi: one of the other review ha mention that after watch just 1 oz episod you ll be hook they are right as thi is exactli what happen with me br br the first thing that struck me about oz wa it brutal and unflinch scene of violenc which set in right from the word go trust me thi is not show for the faint heart or timid thi show pull no punch with regard to drug sex or violenc it is hardcor in the classic use of the word br br it is call oz as that is the nicknam given to the oswald maximum secur state penitentari it focus mainli on emerald citi an experiment section of the prison where all the cell have glass front and face inward so privaci is not high on the agenda em citi is home to mani aryan muslim gangsta latino christian italian irish and more so scuffl death stare dodgi deal and shadi agreement are never far away br br would say the main appeal of the show is due to the fact that it goe where other show wouldn dare forget pretti pictur paint for mainstream audienc forget charm forget romanc oz doesn mess around the first episod ever saw struck me as so nasti it wa surreal couldn say wa readi for it but as watch more develop tast for oz and got accustom to the high level of graphic violenc not just violenc but injustic crook guard who ll be sold out for nickel inmat who ll kill on order and get away with it well manner middl class inmat be turn into prison bitch due to their lack of street skill or prison experi watch oz you may becom comfort with what is uncomfort view that if you can get in touch with your darker side \n",
      "Sonrası: [  26    3    1   71  350   43  471   10  109   55   38  345 2811  298\n",
      "   18  259   20 1725   29   24  212   14    9    5  681   44  207   16\n",
      "   70    7    7    1   94  100   10 2918   70   41 2811   13    6 1231\n",
      "    2   72    3  618   64  183    8  212   35    1  404   86 1481   70\n",
      "    9    5   21   77   15    1 5023  450   37 8387    9   77  709   60\n",
      " 1786   16 1197    4  725  429   37  618    6    5 3339    8    1  348\n",
      "  147    3    1  404    7    7    6    5  258 2811   14   10    5    1\n",
      " 6465  420    4    1 5547 1947  625    6 1252 1427   19  549   32 3443\n",
      " 2023    3    1  888  123   27    1 2009   22 1904  991    2  314   34\n",
      "    5   21  336   19    1 3790 2666  549    5  383    4  115 3545 9334\n",
      " 5744 1184 1033 2288    2   53   34  340 2180 5869  529    2 6874 6553\n",
      "   24  121  250  272    7    7   63  108    1  311  925    3    1   77\n",
      "    5  735    4    1  202   10    6  304  123   71   77  633 1631  793\n",
      "  203  388 1159   15 2341  285  793  743  793  868 2811  164  839  208\n",
      "    1   94  298  130  232 2918   70   14   34 1519    6   13 1868  476\n",
      "  108   13 1563   15    6   17   14   55   53  482 1056   15 2811    2\n",
      "  205 7806    4    1  336  567    3 1330  618   21   38  618   17 5256\n",
      " 3046 1837   33  259   20 2723   40   15 4471   33  259  200   19  580\n",
      "    2   54  272   16    6   69 1110  695  682 4471   20  192   89  888\n",
      " 4024  735    4   67  414    3  720 1130   37  888  461   55 2811   18\n",
      "  220  238 1950   16   44    5 2764  333   10   42   18   46   54    8\n",
      "  560   16  133 3546  477]\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin içinde geçen kelimelerden 10000 kelimelik bir sözlük oluşturuluyor.\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#tokenizer.word_index\n",
    "\n",
    "#Cümleler sayılara dönüştürülüyor\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#Cümlelerin önceki ve sonraki hallerinin görüntülenmesi\n",
    "IDX = 0\n",
    "print(\"Öncesi: {}\".format(x_train[IDX]))\n",
    "print(\"Sonrası: {}\".format(np.array(x_train_tokens[IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n",
      "% 94.54\n"
     ]
    }
   ],
   "source": [
    "#RNN'e girdileri vermeden önce tamamının aynı boyutta olması gerekli. Bu sebeple aşağıdaki matematiksel işlemleri yapıyoruz.\n",
    "total_sentences = x_train_tokens + x_test_tokens\n",
    "num_tokens = np.array([len(tokens) for tokens in total_sentences])\n",
    "#print(np.mean(num_tokens))\n",
    "#print(np.std(num_tokens))\n",
    "#print(np.max(num_tokens))\n",
    "#print(np.min(num_tokens))\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # np.std = standart sapma\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "#Verinin ne kadarını bu kapsama aldığımızın ölçülmesi\n",
    "print(\"%\", round(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding işlemi. Bulunan uzunluk değerine göre cümlelerin yeniden düzenlenmesi. Kısa olanların başına sıfır eklenmesi.\n",
    "#Uzun olanlardan baştan silme yapılması\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
    "x_test_pad  = pad_sequences(x_test_tokens,  maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN oluşturma\n",
    "#ardışık bir model\n",
    "model = Sequential()\n",
    "    \n",
    "#her kelimeye karşılık gelen 50 uzunluğunda bir vektör oluşturulur. (Embedding matrisi)\n",
    "embedding_size = 50\n",
    "    \n",
    "#matris kelime sayısı ve embedding büyüklüğünde olacak, yani 10bine 50 uzunluğunda \n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer'))\n",
    "#LSTM layerlerinin eklenmesi\n",
    "# 16 nöronlu LSTM (16 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "# 8 nöronlu LSTM (8 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "# 4 nöronlu LSTM (4 outputlu , return_sequences=False yani default değer, tek bir output verecek)\n",
    "model.add(GRU(units=4))\n",
    "# Tek bir nörondan oluşan output layer'ı\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#modelin derlenmesi \n",
    "#iki sınıf olduğu için loss fonksiyonu olarak binary_crossentropy \n",
    "#modelin başarısını görmek için accuracy metrics\n",
    "#optimizasyon algoritması\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 545, 50)           500000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 545, 16)           3264      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 545, 8)            624       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 504,061\n",
      "Trainable params: 504,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Modelin özeti\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "176/176 [==============================] - 85s 451ms/step - loss: 0.6133 - accuracy: 0.6389\n",
      "Epoch 2/5\n",
      "176/176 [==============================] - 81s 461ms/step - loss: 0.3074 - accuracy: 0.8799\n",
      "Epoch 3/5\n",
      "176/176 [==============================] - 81s 460ms/step - loss: 0.2189 - accuracy: 0.9209\n",
      "Epoch 4/5\n",
      "176/176 [==============================] - 81s 459ms/step - loss: 0.1816 - accuracy: 0.9381\n",
      "Epoch 5/5\n",
      "176/176 [==============================] - 81s 460ms/step - loss: 0.1534 - accuracy: 0.9516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f302575b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model eğitimi, bir defa eğitimden geçmesi -> epoch , batch_size -> 256'şar 256'şar beslenecek.\n",
    "model.fit(x_train_pad, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 9s 51ms/step - loss: 0.2900 - accuracy: 0.8966\n",
      "Test verisindeki 5000 adet cümleden 4483 tanesi doğru bilindi.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate fonksiyonu yalnızca accuracy ve loss değerini döndürür\n",
    "result = model.evaluate(x_test_pad, y_test)\n",
    "\n",
    "num_true_sentence = int(len(x_test) * result[1])\n",
    "print(\"Test verisindeki {} adet cümleden {} tanesi doğru bilindi.\".format(len(x_test), num_true_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle: what enjoy most in thi film wa the sceneri of corfu be greek ador my countri and like the flatter director point of view base on true stori dure the year when greec wa struggl to stand on her own two feet through war nazi and hardship an italian soldier and greek girl fall in love but the time are hard and they have lot of sacrific to make nichola cage look great in uniform give passion account of thi unfulfil in the begin love ador christian bale play mandra the heroin husband to be he look veri veri good as greek hi person match the one of the greek patriot true fighter in there or what one of the movi would like to buy and keep it in my collect for ever  \n",
      "Asıl Etiket: 1 \n",
      "Üretilen Etiket: 1\n"
     ]
    }
   ],
   "source": [
    "#tek tek cümlelerin sonuçlarını görmek için predict metodu kullanılması\n",
    "y_pred = model.predict(x_test_pad)\n",
    "\n",
    "#Her cümle için çıktı 0 ile 1 arasındadır. 0 olumsuz 1 olumlu anlamındadır. \n",
    "#0.5 üzerini olumlu altını olumsuz olarak işaretleyelim.\n",
    "y_pred = np.array([1 if p>0.5 else 0 for p in y_pred])\n",
    "\n",
    "#Bir örnek üzerinde inceleyelim.\n",
    "IDX = 0\n",
    "sentence = x_test[IDX]\n",
    "real_rate = y_test[IDX]\n",
    "predicted_rate = y_pred[IDX]\n",
    "\n",
    "print(\"Cümle: {} \\nAsıl Etiket: {} \\nÜretilen Etiket: {}\".format(sentence, real_rate, predicted_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2271  259]\n",
      " [ 258 2212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      2530\n",
      "           1       0.90      0.90      0.90      2470\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy değeri hariç precision,recall ve f-measure değerlerine bakalım.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
