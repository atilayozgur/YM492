{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import re\n",
    "import nltk \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinde 50000 adet cümle mevcut.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  One of the other reviewers has mentioned that ...       1\n",
       "1  A wonderful little production. <br /><br />The...       1\n",
       "2  I thought this was a wonderful way to spend ti...       1\n",
       "3  Basically there's a family where a little boy ...       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veri setinin yüklenmesi ve örnek veri\n",
    "dataset = pd.read_csv(\"data.csv\",delimiter=\";\",header=None,names=[\"Review\",\"Rating\"])\n",
    "print(\"Verisetinde {} adet cümle mevcut.\".format(len(dataset)))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. \n",
      "\n",
      "\n",
      "one of the other review ha mention that after watch just 1 oz episod you 'll be hook . they are right , as thi is exactli what happen with me. < br / > < br / > the first thing that struck me about oz wa it brutal and unflinch scene of violenc , which set in right from the word go . trust me , thi is not a show for the faint heart or timid . thi show pull no punch with regard to drug , sex or violenc . it is hardcor , in the classic use of the word. < br / > < br / > it is call oz as that is the nicknam given to the oswald maximum secur state penitentari . it focus mainli on emerald citi , an experiment section of the prison where all the cell have glass front and face inward , so privaci is not high on the agenda . em citi is home to mani .. aryan , muslim , gangsta , latino , christian , italian , irish and more .... so scuffl , death stare , dodgi deal and shadi agreement are never far away. < br / > < br / > i would say the main appeal of the show is due to the fact that it goe where other show would n't dare . forget pretti pictur paint for mainstream audienc , forget charm , forget romanc ... oz doe n't mess around . the first episod i ever saw struck me as so nasti it wa surreal , i could n't say i wa readi for it , but as i watch more , i develop a tast for oz , and got accustom to the high level of graphic violenc . not just violenc , but injustic ( crook guard who 'll be sold out for a nickel , inmat who 'll kill on order and get away with it , well manner , middl class inmat be turn into prison bitch due to their lack of street skill or prison experi ) watch oz , you may becom comfort with what is uncomfort view .... that if you can get in touch with your darker side . \n"
     ]
    }
   ],
   "source": [
    "#Veri ön işleme\n",
    "#Ön işleme öncesi örnek cümle\n",
    "print(dataset['Review'].values[0],\"\\n\\n\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Stemmer nesnesi oluşturulması\n",
    "porter = PorterStemmer() \n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#büyük harflerin küçük harfe çevrilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "\n",
    "#stemming işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: stemSentence(x))\n",
    "\n",
    "data = dataset['Review'].values.tolist()\n",
    "target = dataset['Rating'].values.tolist()\n",
    "target = np.array(target)\n",
    "\n",
    "#Ön işleme sonrası aynı cümle\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öncesi: one of the other review ha mention that after watch just 1 oz episod you 'll be hook . they are right , as thi is exactli what happen with me. < br / > < br / > the first thing that struck me about oz wa it brutal and unflinch scene of violenc , which set in right from the word go . trust me , thi is not a show for the faint heart or timid . thi show pull no punch with regard to drug , sex or violenc . it is hardcor , in the classic use of the word. < br / > < br / > it is call oz as that is the nicknam given to the oswald maximum secur state penitentari . it focus mainli on emerald citi , an experiment section of the prison where all the cell have glass front and face inward , so privaci is not high on the agenda . em citi is home to mani .. aryan , muslim , gangsta , latino , christian , italian , irish and more .... so scuffl , death stare , dodgi deal and shadi agreement are never far away. < br / > < br / > i would say the main appeal of the show is due to the fact that it goe where other show would n't dare . forget pretti pictur paint for mainstream audienc , forget charm , forget romanc ... oz doe n't mess around . the first episod i ever saw struck me as so nasti it wa surreal , i could n't say i wa readi for it , but as i watch more , i develop a tast for oz , and got accustom to the high level of graphic violenc . not just violenc , but injustic ( crook guard who 'll be sold out for a nickel , inmat who 'll kill on order and get away with it , well manner , middl class inmat be turn into prison bitch due to their lack of street skill or prison experi ) watch oz , you may becom comfort with what is uncomfort view .... that if you can get in touch with your darker side . \n",
      "Sonrası: [  31    4    1   79  347   48  474   12  116   60   44  346 2834  299\n",
      "   21  261   24 1746   34   28  212   17   11    6  676   50  209   19\n",
      "   77    8    8    1  102  108   12 2908   77   47 2834   15    7 1236\n",
      "    2   81    4  644   70  183    9  212   40    1  403   94 1490   77\n",
      "   11    6   26    3   85   18    1 5059  446   43 8509   11   85  704\n",
      "   66 1795   19 1201    5  731  428   43  644    7    6 3405    9    1\n",
      "  351  152    4    1  403    8    8    7    6  273 2834   17   12    6\n",
      "    1 6501  416    5    1 5591 1974  628    7 1227 1436   22  557   37\n",
      " 3416 2088    4    1  885  129   33    1 2035   27 1922  975    2  336\n",
      "   39    6   26  332   22    1 3795 5747  557    6  386    5  121 3563\n",
      " 9755 5773 1187 1026 2277    2   56   39  342 2195 5985  523    2 6977\n",
      " 6598   28  128  250  270    8    8   10   64  115    1  310  923    4\n",
      "    1   85    6  727    5    1  199   12    7  303  129   79   85   64\n",
      "   25 1643  785  204  397 1172   18 2357  291  785  748  785  896 2834\n",
      "   82   25  839  205    1  102  299   10  136  232 2908   77   17   39\n",
      " 1529    7   15 1902   10   90   25  115   10   15 1595   18    7   20\n",
      "   17   10   60   56   10  481    3 1058   18 2834    2  203 7976    5\n",
      "    1  332  569    4 1341  644   26   44  644   20 5462 3061 1850   38\n",
      "  261   24 2719   46   18    3 4572   38  261  201   22  580    2   58\n",
      "  270   19    7   76 1135  813  675 4572   24  193   98  885 4050  727\n",
      "    5   73  413    4  720 1136   43  885  470   60 2834   21  221  238\n",
      " 1959   19   50    6 2791  334   12   49   21   69   58    9  559   19\n",
      "  138 3564  488]\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin içinde geçen kelimelerden 10000 kelimelik bir sözlük oluşturuluyor.\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#tokenizer.word_index\n",
    "\n",
    "#Cümleler sayılara dönüştürülüyor\n",
    "data_tokens = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#Cümlelerin önceki ve sonraki hallerinin görüntülenmesi\n",
    "IDX = 0\n",
    "print(\"Öncesi: {}\".format(data[IDX]))\n",
    "print(\"Sonrası: {}\".format(np.array(data_tokens[IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "% 94.52\n"
     ]
    }
   ],
   "source": [
    "#RNN'e girdileri vermeden önce tamamının aynı boyutta olması gerekli. Bu sebeple aşağıdaki matematiksel işlemleri yapıyoruz.\n",
    "\n",
    "num_tokens = np.array([len(tokens) for tokens in data_tokens])\n",
    "#print(np.mean(num_tokens))\n",
    "#print(np.std(num_tokens))\n",
    "#print(np.max(num_tokens))\n",
    "#print(np.min(num_tokens))\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # np.std = standart sapma\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "#Verinin ne kadarını bu kapsama aldığımızın ölçülmesi\n",
    "print(\"%\", round(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding işlemi. Bulunan uzunluk değerine göre cümlelerin yeniden düzenlenmesi. Kısa olanların başına sıfır eklenmesi.\n",
    "#Uzun olanlardan baştan silme yapılması\n",
    "data_pad = pad_sequences(data_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeli oluşuran fonksiyon, KerasClassifier oluşturmak için gerekli\n",
    "def create_model():\n",
    "    #RNN oluşturma, ardışık bir model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #her kelimeye karşılık gelen 50 uzunluğunda bir vektör oluşturulur. (Embedding matrisi)\n",
    "    embedding_size = 50\n",
    "    \n",
    "    #matris kelime sayısı ve embedding büyüklüğünde olacak, yani 10bine 50 uzunluğunda \n",
    "    model.add(Embedding(input_dim=num_words,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=max_tokens,\n",
    "                        name='embedding_layer'))\n",
    "    #LSTM layerlerinin eklenmesi\n",
    "    # 16 nöronlu LSTM (16 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=16, return_sequences=True))\n",
    "    # 8 nöronlu LSTM (8 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=8, return_sequences=True))\n",
    "    # 4 nöronlu LSTM (4 outputlu , return_sequences=False yani default değer, tek bir output verecek)\n",
    "    model.add(GRU(units=4))\n",
    "    # Tek bir nörondan oluşan output layer'ı\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #modelin derlenmesi \n",
    "    #iki sınıf olduğu için loss fonksiyonu olarak binary_crossentropy \n",
    "    #modelin başarısını görmek için accuracy metrics\n",
    "    #optimizasyon algoritması\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy','Precision','Recall',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 15s 86ms/step - loss: 0.3178 - accuracy: 0.8848 - precision: 0.8653 - recall: 0.9056\n",
      "Accuracy=  0.8848000168800354\n",
      "Precision=  0.8652870059013367\n",
      "Recall=  0.9055987000465393\n",
      "F-measure=  0.8849840337854951\n"
     ]
    }
   ],
   "source": [
    "# Modelin değerlendirilmesi\n",
    "seed=0\n",
    "#hold-out \n",
    "x_train, x_test, y_train, y_test = train_test_split(data_pad, target, test_size=0.1, random_state=seed)\n",
    "model=create_model()\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "\n",
    "#Evaluate fonksiyonu yalnızca accuracy ve loss değerini döndürür\n",
    "result = model.evaluate(x_test, y_test)\n",
    "\n",
    "import statistics\n",
    "dizi = [result[2],result[3]]\n",
    "\n",
    "print(\"Accuracy= \",result[1])\n",
    "print(\"Precision= \",result[2])\n",
    "print(\"Recall= \",result[3])\n",
    "print(\"F-measure= \",statistics.harmonic_mean(dizi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modelin oluşturulması\n",
    "# Model eğitimi, bir defa eğitimden geçmesi -> epoch , batch_size -> 256'şar 256'şar beslenecek.\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=256, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8893000000000001  Standart Deviation=  0.00438657041434422\n",
      "Precision=  0.8952326591526731  Standart Deviation=  0.012289017436820853\n",
      "Recall=  0.8822442328429478  Standart Deviation=  0.017884098415655282\n",
      "F-measure=  0.8884509232852696  Standart Deviation=  0.004813676913180402\n"
     ]
    }
   ],
   "source": [
    "#k-fold cross validation\n",
    "scoring = ['accuracy', 'precision','recall','f1']\n",
    "kfold = KFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=kfold, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8876000000000002  Standart Deviation=  0.0036122015447646256\n",
      "Precision=  0.8815367683206876  Standart Deviation=  0.019426266768647033\n",
      "Recall=  0.8968400000000001  Standart Deviation=  0.025919537032902402\n",
      "F-measure=  0.8885612579350959  Standart Deviation=  0.004505465477292743\n"
     ]
    }
   ],
   "source": [
    "#stratified k-fold validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=skfold, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#leave-one-out cross validation\n",
    "#Don’t Use LOOCV: Large datasets or costly models to fit(e.g. neural networks).\n",
    "loo = LeaveOneOut()\n",
    "results = cross_val_score(model, data_pad, target, cv=loo, n_jobs=-1)\n",
    "print(results.mean())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
