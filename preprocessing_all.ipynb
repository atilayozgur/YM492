{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import re\n",
    "import nltk \n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinde 50000 adet cümle mevcut.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  One of the other reviewers has mentioned that ...       1\n",
       "1  A wonderful little production. <br /><br />The...       1\n",
       "2  I thought this was a wonderful way to spend ti...       1\n",
       "3  Basically there's a family where a little boy ...       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veri setinin yüklenmesi ve örnek veri\n",
    "dataset = pd.read_csv(\"data.csv\",delimiter=\";\",header=None,names=[\"Review\",\"Rating\"])\n",
    "print(\"Verisetinde {} adet cümle mevcut.\".format(len(dataset)))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. \n",
      "\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "one review mention watch 1 oz episod hook right exactli happen br br first thing strike oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word br br call oz nicknam give oswald maximum secur state penitentari focu mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death star dodgi deal shadi agreement never far away br br would say main appeal show due fact go show dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw strike nasti surreal say readi watch develop tast oz get accustom high level graphic violenc violenc injustic crook guard sell nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side \n"
     ]
    }
   ],
   "source": [
    "#Veri ön işleme\n",
    "#Ön işleme öncesi örnek cümle\n",
    "print(dataset['Review'].values[0],\"\\n\\n\")\n",
    "\n",
    "#stopwords ön hazırlık\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "print(stop_word_list)\n",
    "\n",
    "def token(values):\n",
    "    words = nltk.tokenize.word_tokenize(values)\n",
    "    filtered_words = [word for word in words if word not in stop_word_list]\n",
    "    not_stopword_doc = \" \".join(filtered_words)\n",
    "    return not_stopword_doc\n",
    "\n",
    "#stemming ön hazırlık\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Stemmer nesnesi oluşturulması\n",
    "porter = PorterStemmer() \n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#lemmatization ön hazırlık\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmaSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    lemma_sentence=[]\n",
    "    for word in token_words:\n",
    "        lemma_sentence.append(wordnet_lemmatizer.lemmatize(word,pos='v'))\n",
    "        lemma_sentence.append(\" \")\n",
    "    return \"\".join(lemma_sentence)\n",
    "\n",
    "#büyük harflerin küçük harfe çevrilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "\n",
    "# Özel karakterlerin(noktalama işareti vs) çıkartılması\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\W\", \" \", x))\n",
    "\n",
    "# tek karakterlerin boşluk ile değiştirilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", x))\n",
    "\n",
    "# en baştan tek kalan karakterlerin çıkartılması\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", x))\n",
    "\n",
    "# Birden fazla boşluğun tek boşlukla değiştirilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "\n",
    "# b öneklerinin silinmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: re.sub(r\"^b\\s+\", \" \", x))\n",
    "\n",
    "#fazladan boşlukların temizlenmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.strip())\n",
    "\n",
    "#stopwordlerin temizlenmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: token(x))\n",
    "\n",
    "#Lemmatization işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: lemmaSentence(x))\n",
    "\n",
    "#stemming işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: stemSentence(x))\n",
    "                              \n",
    "data = dataset['Review'].values.tolist()\n",
    "target = dataset['Rating'].values.tolist()\n",
    "\n",
    "#Ön işleme sonrası aynı cümle\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 adet cümle eğitim için kullanılacak.\n",
      "5000 adet cümle test için kullanılacak.\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin eğitim ve test olarak ayrılması %90 Eğitim %10 test\n",
    "ratio = int(len(data) * .90)\n",
    "x_train, y_train = data[:ratio], target[:ratio]\n",
    "y_train = np.array(y_train)\n",
    "x_test, y_test   = data[ratio:], target[ratio:]\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"{} adet cümle eğitim için kullanılacak.\".format(len(x_train)))\n",
    "print(\"{} adet cümle test için kullanılacak.\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öncesi: one review mention watch 1 oz episod hook right exactli happen br br first thing strike oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word br br call oz nicknam give oswald maximum secur state penitentari focu mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death star dodgi deal shadi agreement never far away br br would say main appeal show due fact go show dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw strike nasti surreal say readi watch develop tast oz get accustom high level graphic violenc violenc injustic crook guard sell nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side \n",
      "Sonrası: [   4  235  353   12  232 2625  188 1562  115  552  109    1    1   32\n",
      "   39 1022 2625 1088   21  495   89  115  288   13 1319   19 4811  333\n",
      " 8159   19  581 1623 1056  596  314  495 3147  233   62  288    1    1\n",
      "  158 2625 6245   35 5330 1780  500  557 1269  431 3254 1853  755 1841\n",
      " 1737  853  204  224 3592 2482  431  266   47 3360 9101 5528 1043  893\n",
      " 2118  228   75 5652  380 6647 6332   50  150  169    1    1   16   24\n",
      "  202  792   19  607  105   13   19 1465  401  106  271 1019 2168  180\n",
      "  401  616  401  733 2625  705  110   32  188   55  133 1022 1356 1704\n",
      "   24 1397   12  363  915 2625    8 7574  224  448 1178  495  495 5041\n",
      " 2860 1674  899 4263  103  462    8  169   20  968  569  553 4263   96\n",
      "  755 3819  607  299  593  990  755  297   12 2625  121   97 1784 2582\n",
      "  221 1416    8  442 3361  358]\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin içinde geçen kelimelerden 10000 kelimelik bir sözlük oluşturuluyor.\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#tokenizer.word_index\n",
    "\n",
    "#Cümleler sayılara dönüştürülüyor\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#Cümlelerin önceki ve sonraki hallerinin görüntülenmesi\n",
    "IDX = 0\n",
    "print(\"Öncesi: {}\".format(x_train[IDX]))\n",
    "print(\"Sonrası: {}\".format(np.array(x_train_tokens[IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n",
      "% 94.48\n"
     ]
    }
   ],
   "source": [
    "#RNN'e girdileri vermeden önce tamamının aynı boyutta olması gerekli. Bu sebeple aşağıdaki matematiksel işlemleri yapıyoruz.\n",
    "total_sentences = x_train_tokens + x_test_tokens\n",
    "num_tokens = np.array([len(tokens) for tokens in total_sentences])\n",
    "#print(np.mean(num_tokens))\n",
    "#print(np.std(num_tokens))\n",
    "#print(np.max(num_tokens))\n",
    "#print(np.min(num_tokens))\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # np.std = standart sapma\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "#Verinin ne kadarını bu kapsama aldığımızın ölçülmesi\n",
    "print(\"%\", round(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding işlemi. Bulunan uzunluk değerine göre cümlelerin yeniden düzenlenmesi. Kısa olanların başına sıfır eklenmesi.\n",
    "#Uzun olanlardan baştan silme yapılması\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
    "x_test_pad  = pad_sequences(x_test_tokens,  maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN oluşturma\n",
    "#ardışık bir model\n",
    "model = Sequential()\n",
    "    \n",
    "#her kelimeye karşılık gelen 50 uzunluğunda bir vektör oluşturulur. (Embedding matrisi)\n",
    "embedding_size = 50\n",
    "    \n",
    "#matris kelime sayısı ve embedding büyüklüğünde olacak, yani 10bine 50 uzunluğunda \n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer'))\n",
    "#LSTM layerlerinin eklenmesi\n",
    "# 16 nöronlu LSTM (16 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "# 8 nöronlu LSTM (8 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "# 4 nöronlu LSTM (4 outputlu , return_sequences=False yani default değer, tek bir output verecek)\n",
    "model.add(GRU(units=4))\n",
    "# Tek bir nörondan oluşan output layer'ı\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#modelin derlenmesi \n",
    "#iki sınıf olduğu için loss fonksiyonu olarak binary_crossentropy \n",
    "#modelin başarısını görmek için accuracy metrics\n",
    "#optimizasyon algoritması\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 292, 50)           500000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 292, 16)           3264      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 292, 8)            624       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 504,061\n",
      "Trainable params: 504,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Modelin özeti\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "176/176 [==============================] - 46s 232ms/step - loss: 0.5933 - accuracy: 0.6883\n",
      "Epoch 2/5\n",
      "176/176 [==============================] - 40s 230ms/step - loss: 0.2893 - accuracy: 0.8967\n",
      "Epoch 3/5\n",
      "176/176 [==============================] - 42s 236ms/step - loss: 0.2146 - accuracy: 0.9289\n",
      "Epoch 4/5\n",
      "176/176 [==============================] - 41s 234ms/step - loss: 0.1797 - accuracy: 0.9419\n",
      "Epoch 5/5\n",
      "176/176 [==============================] - 41s 233ms/step - loss: 0.1566 - accuracy: 0.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3217944c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model eğitimi, bir defa eğitimden geçmesi -> epoch , batch_size -> 256'şar 256'şar beslenecek.\n",
    "model.fit(x_train_pad, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 27ms/step - loss: 0.3055 - accuracy: 0.8854\n",
      "Test verisindeki 5000 adet cümleden 4426 tanesi doğru bilindi.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate fonksiyonu yalnızca accuracy ve loss değerini döndürür\n",
    "result = model.evaluate(x_test_pad, y_test)\n",
    "\n",
    "num_true_sentence = int(len(x_test) * result[1])\n",
    "print(\"Test verisindeki {} adet cümleden {} tanesi doğru bilindi.\".format(len(x_test), num_true_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle: enjoy film sceneri corfu greek ador countri like flatter director point view base true stori year greec struggl stand two feet war nazi hardship italian soldier greek girl fall love time hard lot sacrific make nichola cage look great uniform give passion account unfulfil begin love ador christian bale play mandra heroin husband look good greek person match one greek patriot true fighter one movi would like buy keep collect ever  \n",
      "Asıl Etiket: 1 \n",
      "Üretilen Etiket: 1\n"
     ]
    }
   ],
   "source": [
    "#tek tek cümlelerin sonuçlarını görmek için predict metodu kullanılması\n",
    "y_pred = model.predict(x_test_pad)\n",
    "\n",
    "#Her cümle için çıktı 0 ile 1 arasındadır. 0 olumsuz 1 olumlu anlamındadır. \n",
    "#0.5 üzerini olumlu altını olumsuz olarak işaretleyelim.\n",
    "y_pred = np.array([1 if p>0.5 else 0 for p in y_pred])\n",
    "\n",
    "#Bir örnek üzerinde inceleyelim.\n",
    "IDX = 0\n",
    "sentence = x_test[IDX]\n",
    "real_rate = y_test[IDX]\n",
    "predicted_rate = y_pred[IDX]\n",
    "\n",
    "print(\"Cümle: {} \\nAsıl Etiket: {} \\nÜretilen Etiket: {}\".format(sentence, real_rate, predicted_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2193  337]\n",
      " [ 236 2234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      2530\n",
      "           1       0.87      0.90      0.89      2470\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy değeri hariç precision,recall ve f-measure değerlerine bakalım.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
