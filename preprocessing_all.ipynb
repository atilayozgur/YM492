{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import re\n",
    "import nltk \n",
    "\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinde 50000 adet cümle mevcut.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  One of the other reviewers has mentioned that ...       1\n",
       "1  A wonderful little production. <br /><br />The...       1\n",
       "2  I thought this was a wonderful way to spend ti...       1\n",
       "3  Basically there's a family where a little boy ...       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veri setinin yüklenmesi ve örnek veri\n",
    "dataset = pd.read_csv(\"data.csv\",delimiter=\";\",header=None,names=[\"Review\",\"Rating\"])\n",
    "print(\"Verisetinde {} adet cümle mevcut.\".format(len(dataset)))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. \n",
      "\n",
      "\n",
      "one review mention watch 1 oz episod hook right exactli happen me . first thing strike oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word . call oz nicknam give oswald maximum secur state penitentari focu mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani .. aryan muslim gangsta latino christian italian irish .... scuffl death star dodgi deal shadi agreement never far away . would say main appeal show due fact go show would dare forget pretti pictur paint mainstream audienc forget charm forget romanc ... oz mess around first episod ever saw strike nasti surreal could say readi watch develop tast oz get accustom high level graphic violenc violenc injustic ( crook guard sell nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi ) watch oz may becom comfort uncomfort view .... that get touch darker side \n"
     ]
    }
   ],
   "source": [
    "#Veri ön işleme\n",
    "#Ön işleme öncesi örnek cümle\n",
    "print(dataset['Review'].values[0],\"\\n\\n\")\n",
    "\n",
    "#stopwords ön hazırlık\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "stop_word_list.append(\"'ll\")\n",
    "stop_word_list.append(\"n't\")\n",
    "stop_word_list.append(\"br\")\n",
    "stop_word_list.append(\".\")\n",
    "stop_word_list.append(\",\")\n",
    "stop_word_list.append(\"<\")\n",
    "stop_word_list.append(\">\")\n",
    "stop_word_list.append(\"/\")\n",
    "#print(stop_word_list)\n",
    "\n",
    "def token(values):\n",
    "    words = nltk.tokenize.word_tokenize(values)\n",
    "    filtered_words = [word for word in words if word not in stop_word_list]\n",
    "    not_stopword_doc = \" \".join(filtered_words)\n",
    "    return not_stopword_doc\n",
    "\n",
    "#stemming ön hazırlık\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Stemmer nesnesi oluşturulması\n",
    "porter = PorterStemmer() \n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#lemmatization ön hazırlık\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmaSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    lemma_sentence=[]\n",
    "    for word in token_words:\n",
    "        lemma_sentence.append(wordnet_lemmatizer.lemmatize(word,pos='v'))\n",
    "        lemma_sentence.append(\" \")\n",
    "    return \"\".join(lemma_sentence)\n",
    "\n",
    "#büyük harflerin küçük harfe çevrilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "\n",
    "#stopwordlerin temizlenmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: token(x))\n",
    "\n",
    "#Lemmatization işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: lemmaSentence(x))\n",
    "\n",
    "#stemming işlemi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: stemSentence(x))\n",
    "                              \n",
    "data = dataset['Review'].values.tolist()\n",
    "target = dataset['Rating'].values.tolist()\n",
    "target = np.array(target)\n",
    "\n",
    "#Ön işleme sonrası aynı cümle\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öncesi: one review mention watch 1 oz episod hook right exactli happen me . first thing strike oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word . call oz nicknam give oswald maximum secur state penitentari focu mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani .. aryan muslim gangsta latino christian italian irish .... scuffl death star dodgi deal shadi agreement never far away . would say main appeal show due fact go show would dare forget pretti pictur paint mainstream audienc forget charm forget romanc ... oz mess around first episod ever saw strike nasti surreal could say readi watch develop tast oz get accustom high level graphic violenc violenc injustic ( crook guard sell nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi ) watch oz may becom comfort uncomfort view .... that get touch darker side \n",
      "Sonrası: [   4  237  361   12  239 2705  193 1605  117  558  112 1926   34   40\n",
      " 1063 2705 1119   21  522   91  117  289   13 1353   19 4883  337 8303\n",
      "   19  588 1666 1087  607  318  522 3244  243   64  289  174 2705 6339\n",
      "   37 5423 1836  509  572 1307  445 3341 1964  769 1900 1792  857  230\n",
      "  229 3671 5584  445  276   48 3432 9538 5614 1076  912 2155  235   78\n",
      " 5832  384 6796 6434   51  155  173   14   25  205  811   19  615  107\n",
      "   13   19   14 1508  406  110  281 1049 2231  186  406  625  406  761\n",
      " 2705  720  113   34  193   56  138 1063 1397 1771   28   25 1462   12\n",
      "  363  936 2705    8 7688  229  459 1217  522  522 5124 2908 1712  937\n",
      " 4372  108  474    8  173   20 1020  692  557 4372  100  769 3906  615\n",
      "  306  602 1014  769  304   12 2705  126   99 1824 2636  225  926    8\n",
      "  450 3442  376]\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin içinde geçen kelimelerden 10000 kelimelik bir sözlük oluşturuluyor.\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#tokenizer.word_index\n",
    "\n",
    "#Cümleler sayılara dönüştürülüyor\n",
    "data_tokens = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#Cümlelerin önceki ve sonraki hallerinin görüntülenmesi\n",
    "IDX = 0\n",
    "print(\"Öncesi: {}\".format(data[IDX]))\n",
    "print(\"Sonrası: {}\".format(np.array(data_tokens[IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "% 94.52\n"
     ]
    }
   ],
   "source": [
    "#RNN'e girdileri vermeden önce tamamının aynı boyutta olması gerekli. Bu sebeple aşağıdaki matematiksel işlemleri yapıyoruz.\n",
    "\n",
    "num_tokens = np.array([len(tokens) for tokens in data_tokens])\n",
    "#print(np.mean(num_tokens))\n",
    "#print(np.std(num_tokens))\n",
    "#print(np.max(num_tokens))\n",
    "#print(np.min(num_tokens))\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # np.std = standart sapma\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "#Verinin ne kadarını bu kapsama aldığımızın ölçülmesi\n",
    "print(\"%\", round(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding işlemi. Bulunan uzunluk değerine göre cümlelerin yeniden düzenlenmesi. Kısa olanların başına sıfır eklenmesi.\n",
    "#Uzun olanlardan baştan silme yapılması\n",
    "data_pad = pad_sequences(data_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeli oluşuran fonksiyon, KerasClassifier oluşturmak için gerekli\n",
    "def create_model():\n",
    "    #RNN oluşturma, ardışık bir model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #her kelimeye karşılık gelen 50 uzunluğunda bir vektör oluşturulur. (Embedding matrisi)\n",
    "    embedding_size = 50\n",
    "    \n",
    "    #matris kelime sayısı ve embedding büyüklüğünde olacak, yani 10bine 50 uzunluğunda \n",
    "    model.add(Embedding(input_dim=num_words,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=max_tokens,\n",
    "                        name='embedding_layer'))\n",
    "    #LSTM layerlerinin eklenmesi\n",
    "    # 16 nöronlu LSTM (16 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=16, return_sequences=True))\n",
    "    # 8 nöronlu LSTM (8 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=8, return_sequences=True))\n",
    "    # 4 nöronlu LSTM (4 outputlu , return_sequences=False yani default değer, tek bir output verecek)\n",
    "    model.add(GRU(units=4))\n",
    "    # Tek bir nörondan oluşan output layer'ı\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #modelin derlenmesi \n",
    "    #iki sınıf olduğu için loss fonksiyonu olarak binary_crossentropy \n",
    "    #modelin başarısını görmek için accuracy metrics\n",
    "    #optimizasyon algoritması\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy','Precision','Recall',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 24ms/step - loss: 0.3118 - accuracy: 0.8876 - precision: 0.8784 - recall: 0.8942\n",
      "Accuracy=  0.8876000046730042\n",
      "Precision=  0.8783621191978455\n",
      "Recall=  0.8941560983657837\n",
      "F-measure=  0.8861887428539822\n"
     ]
    }
   ],
   "source": [
    "# Modelin değerlendirilmesi\n",
    "seed=0\n",
    "#hold-out \n",
    "x_train, x_test, y_train, y_test = train_test_split(data_pad, target, test_size=0.1, random_state=seed)\n",
    "model=create_model()\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "\n",
    "#Evaluate fonksiyonu yalnızca accuracy ve loss değerini döndürür\n",
    "result = model.evaluate(x_test, y_test)\n",
    "\n",
    "import statistics\n",
    "dizi = [result[2],result[3]]\n",
    "\n",
    "print(\"Accuracy= \",result[1])\n",
    "print(\"Precision= \",result[2])\n",
    "print(\"Recall= \",result[3])\n",
    "print(\"F-measure= \",statistics.harmonic_mean(dizi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelin oluşturulması\n",
    "# Model eğitimi, bir defa eğitimden geçmesi -> epoch , batch_size -> 256'şar 256'şar beslenecek.\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=256, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8865999999999999  Standart Deviation=  0.004955804677345533\n",
      "Precision=  0.8861867033498539  Standart Deviation=  0.01408957500144965\n",
      "Recall=  0.8876150295862228  Standart Deviation=  0.018960198608535428\n",
      "F-measure=  0.8866258515354127  Standart Deviation=  0.005897973810504174\n"
     ]
    }
   ],
   "source": [
    "#k-fold cross validation\n",
    "scoring = ['accuracy', 'precision','recall','f1']\n",
    "kfold = KFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=kfold, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8847799999999999  Standart Deviation=  0.004423075852842682\n",
      "Precision=  0.8818924860046815  Standart Deviation=  0.013301285355943627\n",
      "Recall=  0.8891599999999998  Standart Deviation=  0.019947491070307567\n",
      "F-measure=  0.8852199292697767  Standart Deviation=  0.005404550231402596\n"
     ]
    }
   ],
   "source": [
    "#stratified k-fold validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=skfold, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#leave-one-out cross validation\n",
    "#Don’t Use LOOCV: Large datasets or costly models to fit(e.g. neural networks).\n",
    "loo = LeaveOneOut()\n",
    "results = cross_val_score(model, data_pad, target, cv=loo, n_jobs=-1)\n",
    "print(results.mean())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
