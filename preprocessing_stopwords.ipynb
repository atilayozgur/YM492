{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import re\n",
    "import nltk \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verisetinde 50000 adet cümle mevcut.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  One of the other reviewers has mentioned that ...       1\n",
       "1  A wonderful little production. <br /><br />The...       1\n",
       "2  I thought this was a wonderful way to spend ti...       1\n",
       "3  Basically there's a family where a little boy ...       0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veri setinin yüklenmesi ve örnek veri\n",
    "dataset = pd.read_csv(\"data.csv\",delimiter=\";\",header=None,names=[\"Review\",\"Rating\"])\n",
    "print(\"Verisetinde {} adet cümle mevcut.\".format(len(dataset)))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side. \n",
      "\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"'ll\", \"n't\", 'br', '.', ',', '<', '>', '/']\n",
      "\n",
      "\n",
      " one reviewers mentioned watching 1 oz episode hooked right exactly happened me. first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word. called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many .. aryans muslims gangstas latinos christians italians irish .... scuffles death stares dodgy dealings shady agreements never far away. would say main appeal show due fact goes shows would dare forget pretty pictures painted mainstream audiences forget charm forget romance ... oz mess around first episode ever saw struck nasty surreal could say ready watched developed taste oz got accustomed high levels graphic violence violence injustice ( crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience ) watching oz may become comfortable uncomfortable viewing .... thats get touch darker side\n"
     ]
    }
   ],
   "source": [
    "#Veri ön işleme\n",
    "#Ön işleme öncesi örnek cümle\n",
    "print(dataset['Review'].values[0],\"\\n\\n\")\n",
    "\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "stop_word_list = nltk.corpus.stopwords.words('english')\n",
    "#stopword liste manuel eklemeler\n",
    "stop_word_list.append(\"'ll\")\n",
    "stop_word_list.append(\"n't\")\n",
    "stop_word_list.append(\"br\")\n",
    "stop_word_list.append(\".\")\n",
    "stop_word_list.append(\",\")\n",
    "stop_word_list.append(\"<\")\n",
    "stop_word_list.append(\">\")\n",
    "stop_word_list.append(\"/\")\n",
    "print(stop_word_list)\n",
    "\n",
    "def token(values):\n",
    "    words = nltk.tokenize.word_tokenize(values)\n",
    "    filtered_words = [word for word in words if word not in stop_word_list]\n",
    "    not_stopword_doc = \" \".join(filtered_words)\n",
    "    return not_stopword_doc\n",
    "\n",
    "#büyük harflerin küçük harfe çevrilmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "\n",
    "#stopwordlerin temizlenmesi\n",
    "dataset['Review'] = dataset['Review'].apply(lambda x: token(x))\n",
    "\n",
    "data = dataset['Review'].values.tolist()\n",
    "target = dataset['Rating'].values.tolist()\n",
    "target = np.array(target)\n",
    "\n",
    "#Ön işleme sonrası aynı cümle\n",
    "print(\"\\n\\n\",data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öncesi: one reviewers mentioned watching 1 oz episode hooked right exactly happened me. first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word. called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many .. aryans muslims gangstas latinos christians italians irish .... scuffles death stares dodgy dealings shady agreements never far away. would say main appeal show due fact goes shows would dare forget pretty pictures painted mainstream audiences forget charm forget romance ... oz mess around first episode ever saw struck nasty surreal could say ready watched developed taste oz got accustomed high levels graphic violence violence injustice ( crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience ) watching oz may become comfortable uncomfortable viewing .... thats get touch darker side\n",
      "Sonrası: [   5 1957  946   60  200 3042  287 3111  106  503  480 2036   23   59\n",
      " 3161 3042 5322   52  451  162  106  537   55 1628   41 7851 2227   41\n",
      " 2501 5856 5436 1360  267  451 3729  254  251  537  324 3042  252 6774\n",
      " 2444  934 2613 1307  413 4587 2364 1082 6959 2826  279  194 4815 7063\n",
      "  413  236   34 8114 4965 7731 2354  219 9062 7321 8651   36  124  140\n",
      "    8   51  176 1213   41  571   98  163  170    8 2914  708   87 1192\n",
      " 4091 2430 1111  708 1327  708  758 3042  831   90   23  287   45  112\n",
      " 3161 1508 2143   17   51 1479  185 1305 1157 3042   88  194 1933 2045\n",
      "  451  451 7619 7110 4932 2891 6811  388  518   19  140   14 7555  530\n",
      "  587 6811  539 1082  571  456  784 1914 1082  442   60 3042  102  315\n",
      " 3745 3233  684 1487   19 1106 4038  371]\n"
     ]
    }
   ],
   "source": [
    "#Cümlelerin içinde geçen kelimelerden 10000 kelimelik bir sözlük oluşturuluyor.\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#tokenizer.word_index\n",
    "\n",
    "#Cümleler sayılara dönüştürülüyor\n",
    "data_tokens = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#Cümlelerin önceki ve sonraki hallerinin görüntülenmesi\n",
    "IDX = 0\n",
    "print(\"Öncesi: {}\".format(data[IDX]))\n",
    "print(\"Sonrası: {}\".format(np.array(data_tokens[IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "% 94.51\n"
     ]
    }
   ],
   "source": [
    "#RNN'e girdileri vermeden önce tamamının aynı boyutta olması gerekli. Bu sebeple aşağıdaki matematiksel işlemleri yapıyoruz.\n",
    "\n",
    "num_tokens = np.array([len(tokens) for tokens in data_tokens])\n",
    "#print(np.mean(num_tokens))\n",
    "#print(np.std(num_tokens))\n",
    "#print(np.max(num_tokens))\n",
    "#print(np.min(num_tokens))\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # np.std = standart sapma\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "#Verinin ne kadarını bu kapsama aldığımızın ölçülmesi\n",
    "print(\"%\", round(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding işlemi. Bulunan uzunluk değerine göre cümlelerin yeniden düzenlenmesi. Kısa olanların başına sıfır eklenmesi.\n",
    "#Uzun olanlardan baştan silme yapılması\n",
    "data_pad = pad_sequences(data_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeli oluşuran fonksiyon, KerasClassifier oluşturmak için gerekli\n",
    "def create_model():\n",
    "    #RNN oluşturma, ardışık bir model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #her kelimeye karşılık gelen 50 uzunluğunda bir vektör oluşturulur. (Embedding matrisi)\n",
    "    embedding_size = 50\n",
    "    \n",
    "    #matris kelime sayısı ve embedding büyüklüğünde olacak, yani 10bine 50 uzunluğunda \n",
    "    model.add(Embedding(input_dim=num_words,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=max_tokens,\n",
    "                        name='embedding_layer'))\n",
    "    #LSTM layerlerinin eklenmesi\n",
    "    # 16 nöronlu LSTM (16 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=16, return_sequences=True))\n",
    "    # 8 nöronlu LSTM (8 outputlu , return_sequences=True demek output'un tamamını ver demek)\n",
    "    model.add(GRU(units=8, return_sequences=True))\n",
    "    # 4 nöronlu LSTM (4 outputlu , return_sequences=False yani default değer, tek bir output verecek)\n",
    "    model.add(GRU(units=4))\n",
    "    # Tek bir nörondan oluşan output layer'ı\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #modelin derlenmesi \n",
    "    #iki sınıf olduğu için loss fonksiyonu olarak binary_crossentropy \n",
    "    #modelin başarısını görmek için accuracy metrics\n",
    "    #optimizasyon algoritması\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy','Precision','Recall',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 25ms/step - loss: 0.3220 - accuracy: 0.8844 - precision: 0.8576 - recall: 0.9158: 1s - loss: 0.3261 - accuracy: 0.8836 - precisio\n",
      "Accuracy=  0.8844000101089478\n",
      "Precision=  0.8576349020004272\n",
      "Recall=  0.9158152937889099\n",
      "F-measure=  0.8857707553377989\n"
     ]
    }
   ],
   "source": [
    "# Modelin değerlendirilmesi\n",
    "seed=0\n",
    "#hold-out \n",
    "x_train, x_test, y_train, y_test = train_test_split(data_pad, target, test_size=0.1, random_state=seed)\n",
    "model=create_model()\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=256, verbose=0)\n",
    "\n",
    "#Evaluate fonksiyonu yalnızca accuracy ve loss değerini döndürür\n",
    "result = model.evaluate(x_test, y_test)\n",
    "\n",
    "import statistics\n",
    "dizi = [result[2],result[3]]\n",
    "\n",
    "print(\"Accuracy= \",result[1])\n",
    "print(\"Precision= \",result[2])\n",
    "print(\"Recall= \",result[3])\n",
    "print(\"F-measure= \",statistics.harmonic_mean(dizi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelin oluşturulması\n",
    "# Model eğitimi, bir defa eğitimden geçmesi -> epoch , batch_size -> 256'şar 256'şar beslenecek.\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=256, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.8836999999999999  Standart Deviation=  0.005438565987463975\n",
      "Precision=  0.8849456425225568  Standart Deviation=  0.019067416979934262\n",
      "Recall=  0.882816073384344  Standart Deviation=  0.01655855785094243\n",
      "F-measure=  0.8835554373431075  Standart Deviation=  0.00540148113925532\n"
     ]
    }
   ],
   "source": [
    "#k-fold cross validation\n",
    "scoring = ['accuracy', 'precision','recall','f1']\n",
    "kfold = KFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=kfold, n_jobs=-1, scoring=scoring)\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=  0.88508  Standart Deviation=  0.005518115620390722\n",
      "Precision=  0.8826953300520648  Standart Deviation=  0.014725320218195061\n",
      "Recall=  0.8888000000000001  Standart Deviation=  0.01694886426873495\n"
     ]
    }
   ],
   "source": [
    "#stratified k-fold validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=seed)\n",
    "results = cross_validate(model, data_pad, target, cv=skfold, n_jobs=-1, scoring=scoring)\n",
    "print(\"Accuracy= \",results['test_accuracy'].mean(),\" Standart Deviation= \", results['test_accuracy'].std())\n",
    "print(\"Precision= \",results['test_precision'].mean(),\" Standart Deviation= \", results['test_precision'].std())\n",
    "print(\"Recall= \",results['test_recall'].mean(),\" Standart Deviation= \", results['test_recall'].std())\n",
    "print(\"F-measure= \",results['test_f1'].mean(),\" Standart Deviation= \", results['test_f1'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave-one-out cross validation\n",
    "#Don’t Use LOOCV: Large datasets or costly models to fit(e.g. neural networks).\n",
    "loo = LeaveOneOut()\n",
    "results = cross_val_score(model, data_pad, target, cv=loo, n_jobs=-1)\n",
    "print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
